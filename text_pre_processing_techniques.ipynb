{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08da9cdf",
   "metadata": {},
   "source": [
    "## Text pre-processing techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a4b18d",
   "metadata": {},
   "source": [
    "## 01) Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9e3f3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Srivathsan\n",
      "[nltk_data]     V\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Srivathsan\n",
      "[nltk_data]     V\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Srivathsan\n",
      "[nltk_data]     V\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Srivathsan V\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Srivathsan\n",
      "[nltk_data]     V\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Srivathsan\n",
      "[nltk_data]     V\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy==3.5.4 in c:\\anc\\lib\\site-packages (3.5.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (0.9.4)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (0.11.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (1.21.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (1.10.22)\n",
      "Requirement already satisfied: jinja2 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (2.11.3)\n",
      "Requirement already satisfied: setuptools in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\anc\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.5.4) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\anc\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy==3.5.4) (4.14.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\anc\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.4) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anc\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.4) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anc\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.4) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anc\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.4) (2022.9.14)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\anc\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy==3.5.4) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\anc\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy==3.5.4) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\anc\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy==3.5.4) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\anc\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy==3.5.4) (8.0.4)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\anc\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.5.4) (1.2.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\anc\\lib\\site-packages (from packaging>=20.0->spacy==3.5.4) (3.0.9)\n",
      "Requirement already satisfied: pathlib-abc==0.1.1 in c:\\anc\\lib\\site-packages (from pathy>=0.10.0->spacy==3.5.4) (0.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\anc\\lib\\site-packages (from jinja2->spacy==3.5.4) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')        # For tokenization\n",
    "nltk.download('stopwords')    # For stop words\n",
    "nltk.download('wordnet')      # For lemmatization\n",
    "nltk.download('averaged_perceptron_tagger') # For POS tagging\n",
    "nltk.download('omw-1.4')      # For WordNet data\n",
    "nltk.download('punkt_tab')    # Download the missing resource for sentence tokenization\n",
    "\n",
    "\n",
    "# Download spaCy model\n",
    "# !python -m spacy download en_core_web_  #spacy is an alternative for nltk\n",
    "!C:\\anc\\python.exe -m pip install spacy==3.5.4\n",
    "# use it if you are getting some errors when downloading spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece9bf9",
   "metadata": {},
   "source": [
    "## 02) Load sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6e36add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text representing a part of a document\n",
    "sample_document = \"\"\"\n",
    "Machine learning (ML) is a field of study in artificial intelligence concerned with the development of computer algorithms that can learn from and make predictions on data. Algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task.\n",
    "\n",
    "Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks. A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers. Mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.\n",
    "\n",
    "In its application across business problems, machine learning is also referred to as predictive analytics.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7414119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also add some text with punctuation, numbers, and mixed casing\n",
    "noisy_text = \"\"\"\n",
    "Machine learning rocks! It's revolutionizing the world in 2023 (and beyond!).\n",
    "Visit our site: http://example.com for more info.\n",
    "This is awesome!!! We collected 1,234 data points.\n",
    "Softbank and Google are major players.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ef7f337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample Document ---\n",
      "\n",
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development of computer algorithms that can learn from and make predictions on data. Algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task.\n",
      "\n",
      "Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks. A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers. Mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.\n",
      "\n",
      "In its application across business problems, machine learning is also referred to as predictive analytics.\n",
      "\n",
      "\n",
      "--- Noisy Text ---\n",
      "\n",
      "Machine learning rocks! It's revolutionizing the world in 2023 (and beyond!).\n",
      "Visit our site: http://example.com for more info.\n",
      "This is awesome!!! We collected 1,234 data points.\n",
      "Softbank and Google are major players.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Sample Document ---\")\n",
    "print(sample_document)\n",
    "print(\"\\n--- Noisy Text ---\")\n",
    "print(noisy_text)\n",
    "\n",
    "\n",
    "######\n",
    "### Note: For loading .docx files, you would typically do:\n",
    "\n",
    "# import docx2txt\n",
    "# try:\n",
    "#     text = docx2txt.process(\"your_document.docx\")\n",
    "#     print(text)\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading docx: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a0c7ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Raw Text Length: 1254\n",
      "--- --------------------------- ---\n",
      "--- Original Raw Text ---\n",
      "\n",
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development of computer algorithms that can learn from and make predictions on data. Algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task.\n",
      "\n",
      "Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks. A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers. Mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.\n",
      "\n",
      "In its application across business problems, machine learning is also referred to as predictive analytics.\n",
      "\n",
      "\n",
      "Machine learning rocks! It's revolutionizing the world in 2023 (and beyond!).\n",
      "Visit our site: http://example.com for more info.\n",
      "This is awesome!!! We collected 1,234 data points.\n",
      "Softbank and Google are major players.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Combine texts\n",
    "raw_text = sample_document + \"\\n\" + noisy_text\n",
    "\n",
    "print(\"Original Raw Text Length:\", len(raw_text))\n",
    "print(\"--- --------------------------- ---\")\n",
    "print(\"--- Original Raw Text ---\")\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a46d2af",
   "metadata": {},
   "source": [
    "## 03) Convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d882052a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- After Lowercasing ---\n",
      "\n",
      "machine learning (ml) is a field of study in artificial intelligence concerned with the development of computer algorithms that can learn from and make predictions on data. algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task.\n",
      "\n",
      "machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks. a subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers. mathematical optimization delivers methods, theory and application domains to the field of machine learning. data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.\n",
      "\n",
      "in its application across business problems, machine learning is also referred to as predictive analytics.\n",
      "\n",
      "\n",
      "machine learning rocks! it's revolutionizing the world in 2023 (and beyond!).\n",
      "visit our site: http://example.com for more info.\n",
      "this is awesome!!! we collected 1,234 data points.\n",
      "softbank and google are major players.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = raw_text.lower()\n",
    "\n",
    "print(\"\\n--- After Lowercasing ---\")\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400a8a72",
   "metadata": {},
   "source": [
    "## 04) Remove or fetch URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba3f5f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "machine learning (ml) is a field of study in artificial intelligence concerned with the development of computer algorithms that can learn from and make predictions on data. algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task.\n",
      "\n",
      "machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks. a subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers. mathematical optimization delivers methods, theory and application domains to the field of machine learning. data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.\n",
      "\n",
      "in its application across business problems, machine learning is also referred to as predictive analytics.\n",
      "\n",
      "\n",
      "machine learning rocks! it's revolutionizing the world in 2023 (and beyond!).\n",
      "visit our site:  for more info.\n",
      "this is awesome!!! we collected 1,234 data points.\n",
      "softbank and google are major players.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    return re.sub(url_pattern, '', text)\n",
    "\n",
    "print(remove_urls(cleaned_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40adff0",
   "metadata": {},
   "source": [
    "## 05) Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1749d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2d5c1t34n\n"
     ]
    }
   ],
   "source": [
    "# string.punctuation includes -> !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# This line creates a translation table that tells Python which characters to delete from a string.\n",
    "\n",
    "# str.maketrans(from, to, delete) creates a mapping for str.translate().\n",
    "\n",
    "#In this case:\n",
    "\n",
    "# from = '' and to = '': no character substitutions.\n",
    "\n",
    "# delete = string.punctuation: remove all punctuation characters.\n",
    "\n",
    "trans = str.maketrans(\"aeiou\", \"12345\")\n",
    "\n",
    "# Explanation:\n",
    "#\"a\" → \"1\", \"e\" → \"2\", \"i\" → \"3\", \"o\" → \"4\", \"u\" → \"5\"\n",
    "\n",
    "text = \"education\"\n",
    "translated = text.translate(trans)\n",
    "print(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6f0d1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- After Removing Punctuation ---\n",
      "\n",
      "machine learning ml is a field of study in artificial intelligence concerned with the development of computer algorithms that can learn from and make predictions on data algorithms build a mathematical model based on sample data known as training data in order to make predictions or decisions without being explicitly programmed to perform the task\n",
      "\n",
      "machine learning algorithms are used in a wide variety of applications such as email filtering and computer vision where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks a subset of machine learning is closely related to computational statistics which focuses on making predictions using computers mathematical optimization delivers methods theory and application domains to the field of machine learning data mining is a related field of study focusing on exploratory data analysis through unsupervised learning\n",
      "\n",
      "in its application across business problems machine learning is also referred to as predictive analytics\n",
      "\n",
      "\n",
      "machine learning rocks its revolutionizing the world in 2023 and beyond\n",
      "visit our site httpexamplecom for more info\n",
      "this is awesome we collected 1234 data points\n",
      "softbank and google are major players\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = cleaned_text.translate(translator)\n",
    "print(\"\\n--- After Removing Punctuation ---\")\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aff055",
   "metadata": {},
   "source": [
    "## 06) Remove or fetch numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cf9bc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- After Removing Numbers ---\n",
      "\n",
      "machine learning ml is a field of study in artificial intelligence concerned with the development of computer algorithms that can learn from and make predictions on data algorithms build a mathematical model based on sample data known as training data in order to make predictions or decisions without being explicitly programmed to perform the task\n",
      "\n",
      "machine learning algorithms are used in a wide variety of applications such as email filtering and computer vision where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks a subset of machine learning is closely related to computational statistics which focuses on making predictions using computers mathematical optimization delivers methods theory and application domains to the field of machine learning data mining is a related field of study focusing on exploratory data analysis through unsupervised learning\n",
      "\n",
      "in its application across business problems machine learning is also referred to as predictive analytics\n",
      "\n",
      "\n",
      "machine learning rocks its revolutionizing the world in  and beyond\n",
      "visit our site httpexamplecom for more info\n",
      "this is awesome we collected  data points\n",
      "softbank and google are major players\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "re.sub(pattern, replacement, text)\n",
    "→ Substitutes all matches of the pattern with the replacement string ('' here).\n",
    "\"\"\"\n",
    "\n",
    "cleaned_text = re.sub(r'\\d+', '', cleaned_text)#Removes all numbers (whole or part of numbers) from the text.\n",
    "\n",
    "\n",
    "print(\"\\n--- After Removing Numbers ---\")\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a559cba1",
   "metadata": {},
   "source": [
    "## 07) Remove extra white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e853a6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- After Removing Extra Whitespace ---\n",
      "machine learning ml is a field of study in artificial intelligence concerned with the development of computer algorithms that can learn from and make predictions on data algorithms build a mathematical model based on sample data known as training data in order to make predictions or decisions without being explicitly programmed to perform the task machine learning algorithms are used in a wide variety of applications such as email filtering and computer vision where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks a subset of machine learning is closely related to computational statistics which focuses on making predictions using computers mathematical optimization delivers methods theory and application domains to the field of machine learning data mining is a related field of study focusing on exploratory data analysis through unsupervised learning in its application across business problems machine learning is also referred to as predictive analytics machine learning rocks its revolutionizing the world in and beyond visit our site httpexamplecom for more info this is awesome we collected data points softbank and google are major players\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "print(\"\\n--- After Removing Extra Whitespace ---\")\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "448df574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Text Length: 1201\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCleaned Text Length:\", len(cleaned_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a123518",
   "metadata": {},
   "source": [
    "## 08) Sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9e91f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sentence Tokenization (NLTK) ---\n",
      "Sentence 1: \n",
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development of computer algorithms that can learn from and make predictions on data.\n",
      "Sentence 2: Algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task.\n",
      "Sentence 3: Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks.\n",
      "Sentence 4: A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers.\n",
      "Sentence 5: Mathematical optimization delivers methods, theory and application domains to the field of machine learning.\n",
      "Sentence 6: Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.\n",
      "Sentence 7: In its application across business problems, machine learning is also referred to as predictive analytics.\n",
      "Sentence 8: Machine learning rocks!\n",
      "Sentence 9: It's revolutionizing the world in 2023 (and beyond!).\n",
      "Sentence 10: Visit our site: http://example.com for more info.\n",
      "Sentence 11: This is awesome!!!\n",
      "Sentence 12: We collected 1,234 data points.\n",
      "Sentence 13: Softbank and Google are major players.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk_sentences = sent_tokenize(raw_text) # Using raw_text to show sentence boundary handling\n",
    "print(\"--- Sentence Tokenization (NLTK) ---\")\n",
    "for i, sentence in enumerate(nltk_sentences):\n",
    "    print(f\"Sentence {i+1}: {sentence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2818c",
   "metadata": {},
   "source": [
    "## 09) Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eafef0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Word Tokenization (NLTK) ---\n",
      "['machine', 'learning', 'ml', 'is', 'a', 'field', 'of', 'study', 'in', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'development', 'of', 'computer', 'algorithms', 'that', 'can']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Using NLTK (on the cleaned text)\n",
    "nltk_word_tokens = word_tokenize(cleaned_text)\n",
    "print(\"--- Word Tokenization (NLTK) ---\")\n",
    "print(nltk_word_tokens[:20]) # Print first 20 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a300f2d8",
   "metadata": {},
   "source": [
    "## 10) Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "748ebaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample NLTK Stop Words ---\n",
      "['against', 'there', 'do', 'such', \"they'd\", \"isn't\", 'are', 'so', 'its', 'needn']\n",
      "\n",
      "--- NLTK Tokens after Stop Word Removal ---\n",
      "['machine', 'learning', 'ml', 'field', 'study', 'artificial', 'intelligence', 'concerned', 'development', 'computer', 'algorithms', 'learn', 'make', 'predictions', 'data', 'algorithms', 'build', 'mathematical', 'model', 'based']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get English stop words from NLTK\n",
    "nltk_stop_words = set(stopwords.words('english'))\n",
    "print(\"--- Sample NLTK Stop Words ---\")\n",
    "print(list(nltk_stop_words)[:10])\n",
    "\n",
    "# Remove stop words using NLTK tokens\n",
    "nltk_tokens_no_stopwords = [word for word in nltk_word_tokens if word not in nltk_stop_words]\n",
    "print(\"\\n--- NLTK Tokens after Stop Word Removal ---\")\n",
    "print(nltk_tokens_no_stopwords[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c933bc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'learning': 7, 'machine': 6, 'data': 6, 'algorithms': 4, 'field': 3, 'predictions': 3, 'study': 2, 'computer': 2, 'make': 2, 'mathematical': 2, 'perform': 2, 'related': 2, 'application': 2, 'ml': 1, 'artificial': 1, 'intelligence': 1, 'concerned': 1, 'development': 1, 'learn': 1, 'build': 1, 'model': 1, 'based': 1, 'sample': 1, 'known': 1, 'training': 1, 'order': 1, 'decisions': 1, 'without': 1, 'explicitly': 1, 'programmed': 1, 'task': 1, 'used': 1, 'wide': 1, 'variety': 1, 'applications': 1, 'email': 1, 'filtering': 1, 'vision': 1, 'difficult': 1, 'infeasible': 1, 'develop': 1, 'conventional': 1, 'needed': 1, 'tasks': 1, 'subset': 1, 'closely': 1, 'computational': 1, 'statistics': 1, 'focuses': 1, 'making': 1, 'using': 1, 'computers': 1, 'optimization': 1, 'delivers': 1, 'methods': 1, 'theory': 1, 'domains': 1, 'mining': 1, 'focusing': 1, 'exploratory': 1, 'analysis': 1, 'unsupervised': 1, 'across': 1, 'business': 1, 'problems': 1, 'also': 1, 'referred': 1, 'predictive': 1, 'analytics': 1, 'rocks': 1, 'revolutionizing': 1, 'world': 1, 'beyond': 1, 'visit': 1, 'site': 1, 'httpexamplecom': 1, 'info': 1, 'awesome': 1, 'collected': 1, 'points': 1, 'softbank': 1, 'google': 1, 'major': 1, 'players': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Step 5: Count word frequencies\n",
    "word_counts = Counter(nltk_tokens_no_stopwords)\n",
    "\n",
    "# Display word counts\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f81132a",
   "metadata": {},
   "source": [
    "## 11) Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6407880f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stemmed Tokens (NLTK PorterStemmer) ---\n",
      "['machin', 'learn', 'ml', 'field', 'studi', 'artifici', 'intellig', 'concern', 'develop', 'comput', 'algorithm', 'learn', 'make', 'predict', 'data', 'algorithm', 'build', 'mathemat', 'model', 'base']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "# --- Stemming with NLTK (PorterStemmer) ---\n",
    "porter_stemmer = PorterStemmer()\n",
    "stemmed_tokens = [porter_stemmer.stem(word) for word in nltk_tokens_no_stopwords] # Apply to NLTK tokens after stop word removal\n",
    "\n",
    "print(\"--- Stemmed Tokens (NLTK PorterStemmer) ---\")\n",
    "print(stemmed_tokens[:20])\n",
    "\n",
    "#Example\n",
    "#    learning -> learn\n",
    "#    machine -> machin (it can sometimes cut of last few letters of the words and thus it can also produce incorrect words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1845e9",
   "metadata": {},
   "source": [
    "## 12) Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81260a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\n",
      "--- Sentence Tokenization (spaCy) ---\n",
      "Sentence 1: \n",
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development of computer algorithms that can learn from and make predictions on data.\n",
      "Sentence 2: Algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task.\n",
      "\n",
      "\n",
      "Sentence 3: Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks.\n",
      "Sentence 4: A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers.\n",
      "Sentence 5: Mathematical optimization delivers methods, theory and application domains to the field of machine learning.\n",
      "Sentence 6: Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.\n",
      "\n",
      "\n",
      "Sentence 7: In its application across business problems, machine learning is also referred to as predictive analytics.\n",
      "\n",
      "\n",
      "\n",
      "Sentence 8: Machine learning rocks!\n",
      "Sentence 9: It's revolutionizing the world in 2023 (and beyond!).\n",
      "\n",
      "Sentence 10: Visit our site: http://example.com for more info.\n",
      "\n",
      "Sentence 11: This is awesome!!!\n",
      "Sentence 12: We collected 1,234 data points.\n",
      "\n",
      "Sentence 13: Softbank and Google are major players.\n",
      "\n",
      "\n",
      "--- Word Tokenization (spaCy) ---\n",
      "['machine', 'learning', 'ml', 'is', 'a', 'field', 'of', 'study', 'in', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'development', 'of', 'computer', 'algorithms', 'that', 'can']\n",
      "\n",
      "--- Lemmatized Tokens (spaCy) ---\n",
      "['machine', 'learn', 'ml', 'field', 'study', 'artificial', 'intelligence', 'concern', 'development', 'computer', 'algorithm', 'learn', 'prediction', 'datum', 'algorithm', 'build', 'mathematical', 'model', 'base', 'sample']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization: A more sophisticated process that uses a vocabulary and morphological analysis to return the\n",
    "# base or dictionary form of a word (known as the lemma).\n",
    "# It's slower but produces actual words (e.g., \"better\" -> \"good\").\n",
    "# Lemmatization often requires the Part-of-Speech (POS) tag of the word to be accurate.\n",
    "\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.corpus import stopwords, wordnet\n",
    "# from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def get_wordnet_pos(treebank_tag):\n",
    "#     \"\"\"Map POS tag to WordNet format.\"\"\"\n",
    "#     if treebank_tag.startswith('J'):\n",
    "#         return wordnet.ADJ\n",
    "#     elif treebank_tag.startswith('V'):\n",
    "#         return wordnet.VERB\n",
    "#     elif treebank_tag.startswith('N'):\n",
    "#         return wordnet.NOUN\n",
    "#     elif treebank_tag.startswith('R'):\n",
    "#         return wordnet.ADV\n",
    "#     else:\n",
    "#         return wordnet.NOUN  # Default POS\n",
    "\n",
    "# # POS tagging\n",
    "# pos_tagged = pos_tag(nltk_tokens_no_stopwords)\n",
    "# print(\"--- pos tag ---\")\n",
    "# print(pos_tagged[:20])\n",
    "\n",
    "# # Lemmatize using POS\n",
    "# nltk_lemmatized_tokens = [\n",
    "#     lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "#     for word, pos in pos_tagged\n",
    "# ]\n",
    "\n",
    "# print(\"\\n--- Lemmatized Tokens (NLTK WordNetLemmatizer) ---\")\n",
    "# print(nltk_lemmatized_tokens[:20])\n",
    "\n",
    "import spacy # altn. for nltk\n",
    "\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_sm\") #downloads the corresponding english spacy model for spacy version\n",
    "\n",
    "# Load the small English spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "spacy_doc_raw = nlp(raw_text) # Using raw_text\n",
    "spacy_sentences = [sent.text for sent in spacy_doc_raw.sents]\n",
    "print(\"\\n--- Sentence Tokenization (spaCy) ---\")\n",
    "for i, sentence in enumerate(spacy_sentences):\n",
    "    print(f\"Sentence {i+1}: {sentence}\")\n",
    "\n",
    "# Using spaCy (on the cleaned text)\n",
    "# spaCy's Doc object already contains tokens\n",
    "spacy_doc_cleaned = nlp(cleaned_text)\n",
    "spacy_word_tokens = [token.text for token in spacy_doc_cleaned]\n",
    "print(\"\\n--- Word Tokenization (spaCy) ---\")\n",
    "print(spacy_word_tokens[:20]) # Print first 20 tokens\n",
    "\n",
    "# --- Lemmatization with spaCy ---\n",
    "\n",
    "# Extract lemmas from spaCy Doc object (on cleaned text, filtering stop words)\n",
    "spacy_lemmatized_tokens = [token.lemma_ for token in spacy_doc_cleaned if not token.is_stop]\n",
    "\n",
    "print(\"\\n--- Lemmatized Tokens (spaCy) ---\")\n",
    "print(spacy_lemmatized_tokens[:20])\n",
    "\n",
    "# Note: spaCy's lemmatizer is generally more accurate as it uses context and\n",
    "# doesn't require manual POS tag conversion like NLTK's WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae37b9d",
   "metadata": {},
   "source": [
    "## 13) Bag of words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d6af87e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos_tag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9588\\3200418477.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;31m# Apply cleaning to all texts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mcleaned_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m# Show result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9588\\3200418477.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;31m# Apply cleaning to all texts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mcleaned_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m# Show result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9588\\3200418477.py\u001b[0m in \u001b[0;36mclean_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# POS tagging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mtagged_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# Lemmatize, remove stopwords, and keep words > 1 char\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pos_tag' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample noisy texts\n",
    "texts = [\n",
    "    \"Machine learning rocks! It's revolutionizing the world in 2023 (and beyond!). Visit our site: http://example.com for more info.\",\n",
    "    \"This is awesome!!! We collected 1,234 data points.\",\n",
    "    \"Softbank and Google are major players.\",\n",
    "    \"Predictive analytics uses machine learning to solve business problems.\",\n",
    "]\n",
    "\n",
    "# Initialize NLTK tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # POS tagging\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "    # Lemmatize, remove stopwords, and keep words > 1 char\n",
    "    lemmatized_tokens = [\n",
    "        lemmatizer.lemmatize(token, get_wordnet_pos(tag))\n",
    "        for token, tag in tagged_tokens\n",
    "        if token not in stop_words and len(token) > 1\n",
    "    ]\n",
    "\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "# Apply cleaning to all texts\n",
    "cleaned_texts = [clean_text(text) for text in texts]\n",
    "\n",
    "# Show result\n",
    "print(\"Cleaned Texts:\")\n",
    "for i, txt in enumerate(cleaned_texts, 1):\n",
    "    print(f\"{i}: {txt}\")\n",
    "\n",
    "# Using sklearn CountVectorizer for BoW\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(cleaned_texts)\n",
    "\n",
    "print(\"\\nBoW Feature Names:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\nBoW Matrix (Count of words per document):\")\n",
    "bow_matrix.toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc3eeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names =vectorizer.get_feature_names_out()\n",
    "bow_values = bow_matrix.toarray()\n",
    "\n",
    "# Create a DataFrame\n",
    "bow_df = pd.DataFrame(bow_values, columns=feature_names)\n",
    "\n",
    "# Add row indices as Document IDs\n",
    "bow_df.index = [f\"Doc_{i+1}\" for i in range(len(bow_df))]\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\nTF-IDF DataFrame:\")\n",
    "bow_df.round(3)  # Optional: round for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a1cc6a",
   "metadata": {},
   "source": [
    "## 14) TF-IDF (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bca4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(cleaned_texts)\n",
    "\n",
    "print(\"TF-IDF Feature Names:\", tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\nTF-IDF Matrix (weights per word per document):\")\n",
    "print(tfidf_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389473ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names and TF-IDF matrix values\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_values = tfidf_matrix.toarray()\n",
    "# Create a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_values, columns=feature_names)\n",
    "\n",
    "# Add row indices as Document IDs\n",
    "tfidf_df.index = [f\"Doc_{i+1}\" for i in range(len(tfidf_df))]\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\nTF-IDF DataFrame:\")\n",
    "tfidf_df.round(3)  # Optional: roun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ce02e5",
   "metadata": {},
   "source": [
    "## 15) Part-of-Speech (POS) Tagging using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec2313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- POS Tagging with spaCy ---\")\n",
    "# spaCy tokens have .pos_ (coarse-grained) and .tag_ (fine-grained) attributes\n",
    "for i, token in enumerate(spacy_doc_cleaned):\n",
    "    print(f\"{token.text}: {token.pos_} ({token.tag_})\")\n",
    "    if i >= 20: break # Limit output\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "# SpaCy's POS tagging is part of its standard pipeline and is generally very accurate.\n",
    "# It provides both coarse-grained (`.pos_`) and fine-grained (`.tag_`) POS tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a28d511",
   "metadata": {},
   "source": [
    "## 16) Shoallow chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc5bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using spaCy to find noun chunks\n",
    "print(\"--- Noun Chunking with spaCy ---\")\n",
    "for chunk in spacy_doc_cleaned.noun_chunks:\n",
    "    print(chunk.text)\n",
    "\n",
    "# SpaCy's noun chunker is a simple form of shallow parsing.\n",
    "# More complex chunking (e.g., identifying verb phrases) can be done by defining patterns\n",
    "# or using libraries specifically for chunking with NLTK or spaCy extensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60ee499",
   "metadata": {},
   "source": [
    "## 17) Named Entity Recognition (NER) \n",
    "It is the process of identifying and classifying named entities in text into predefined categories such as person names, organizations, locations, dates, etc. This is crucial for extracting structured information from unstructured text.\n",
    "\n",
    "SpaCy has excellent built-in NER capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac53277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using spaCy for Named Entity Recognition\n",
    "print(\"--- Named Entity Recognition (NER) with spaCy ---\")\n",
    "for ent in spacy_doc_raw.ents: # Accessing .ents attribute for detected entities\n",
    "    if ent.label_ in ['ORG', 'GPE', 'PERSON', 'DATE', 'CARDINAL', 'LOC']: # Filter for common entity types\n",
    "         print(f\"Entity: {ent.text}, Type: {ent.label_}\")\n",
    "\n",
    "# SpaCy can identify various types of entities.\n",
    "# This is highly valuable for information extraction from documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26505f8",
   "metadata": {},
   "source": [
    "## 18) Vader sentiment analysis\n",
    "How VADER Works \n",
    "\n",
    "Uses a sentiment dictionary:\n",
    "\n",
    "Words like \"great\", \"amazing\" = positive\n",
    "\n",
    "Words like \"bad\", \"terrible\" = negative\n",
    "\n",
    "Applies rules:\n",
    "\n",
    "Boosts scores for ALL CAPS, exclamation marks (!), degree modifiers (\"very\", \"extremely\")\n",
    "\n",
    "Handles negations like \"not good\" → negative\n",
    "\n",
    "Calculates scores:\n",
    "\n",
    "Scores each word\n",
    "\n",
    "Applies modifiers\n",
    "\n",
    "Combines into neg, neu, pos\n",
    "\n",
    "Then computes the final compound score using a normalization formula (-1 to 1).\n",
    "\n",
    "     Compound Score Range\t        Sentiment Interpretation\n",
    "        >= 0.05\t                      Positive sentiment 👍\n",
    "        > -0.05 and < 0.05\t          Neutral sentiment 😐\n",
    "        <= -0.05\t                  Negative sentiment 👎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34d0a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d2b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentiment analysis on a sentence from our original text\n",
    "sentence_for_sentiment = \"Machine learning is a field of study in artificial intelligence.\"\n",
    "vs = analyzer.polarity_scores(sentence_for_sentiment)\n",
    "print(\"\\n--- Sentiment Analysis with NLTK's VADER ---\")\n",
    "print(f\"Sentence: {sentence_for_sentiment}\")\n",
    "print(f\"VADER Polarity Scores: {vs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4028223",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_for_sentiment_2 = \"Machine learning rocks! This is awesome!!!\"\n",
    "vs2 = analyzer.polarity_scores(sentence_for_sentiment_2)\n",
    "print(f\"Sentence: {sentence_for_sentiment_2}\")\n",
    "print(f\"VADER Polarity Scores: {vs2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0610b24c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
