{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08da9cdf",
   "metadata": {},
   "source": [
    "## Text pre-processing techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a4b18d",
   "metadata": {},
   "source": [
    "## 01) Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9e3f3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Srivathsan\n",
      "[nltk_data]     V\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Srivathsan\n",
      "[nltk_data]     V\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Srivathsan\n",
      "[nltk_data]     V\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Srivathsan V\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Srivathsan\n",
      "[nltk_data]     V\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Srivathsan\n",
      "[nltk_data]     V\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy==3.5.4 in c:\\anc\\lib\\site-packages (3.5.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (0.9.4)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (0.11.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (1.21.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (1.10.22)\n",
      "Requirement already satisfied: jinja2 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (2.11.3)\n",
      "Requirement already satisfied: setuptools in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\anc\\lib\\site-packages (from spacy==3.5.4) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\anc\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.5.4) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\anc\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy==3.5.4) (4.14.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\anc\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.4) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anc\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.4) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anc\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.4) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anc\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.4) (2022.9.14)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\anc\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy==3.5.4) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\anc\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy==3.5.4) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\anc\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy==3.5.4) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\anc\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy==3.5.4) (8.0.4)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\anc\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.5.4) (1.2.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\anc\\lib\\site-packages (from packaging>=20.0->spacy==3.5.4) (3.0.9)\n",
      "Requirement already satisfied: pathlib-abc==0.1.1 in c:\\anc\\lib\\site-packages (from pathy>=0.10.0->spacy==3.5.4) (0.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\anc\\lib\\site-packages (from jinja2->spacy==3.5.4) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')        # For tokenization\n",
    "nltk.download('stopwords')    # For stop words\n",
    "nltk.download('wordnet')      # For lemmatization\n",
    "nltk.download('averaged_perceptron_tagger') # For POS tagging\n",
    "nltk.download('omw-1.4')      # For WordNet data\n",
    "nltk.download('punkt_tab')    # Download the missing resource for sentence tokenization\n",
    "\n",
    "\n",
    "# Download spaCy model\n",
    "# !python -m spacy download en_core_web_  #spacy is an alternative for nltk\n",
    "!C:\\anc\\python.exe -m pip install spacy==3.5.4\n",
    "# use it if you are getting some errors when downloading spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece9bf9",
   "metadata": {},
   "source": [
    "## 02) Load sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6e36add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text representing a part of a document\n",
    "sample_document = \"\"\"\n",
    "Machine learning (ML) is a field of study in artificial intelligence concerned with the development of computer algorithms that can learn from and make predictions on data. Algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task.\n",
    "\n",
    "Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks. A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers. Mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.\n",
    "\n",
    "In its application across business problems, machine learning is also referred to as predictive analytics.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7414119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also add some text with punctuation, numbers, and mixed casing\n",
    "noisy_text = \"\"\"\n",
    "Machine learning rocks! It's revolutionizing the world in 2023 (and beyond!).\n",
    "Visit our site: http://example.com for more info.\n",
    "This is awesome!!! We collected 1,234 data points.\n",
    "Softbank and Google are major players.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ef7f337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample Document ---\n",
      "\n",
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development of computer algorithms that can learn from and make predictions on data. Algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task.\n",
      "\n",
      "Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks. A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers. Mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.\n",
      "\n",
      "In its application across business problems, machine learning is also referred to as predictive analytics.\n",
      "\n",
      "\n",
      "--- Noisy Text ---\n",
      "\n",
      "Machine learning rocks! It's revolutionizing the world in 2023 (and beyond!).\n",
      "Visit our site: http://example.com for more info.\n",
      "This is awesome!!! We collected 1,234 data points.\n",
      "Softbank and Google are major players.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Sample Document ---\")\n",
    "print(sample_document)\n",
    "print(\"\\n--- Noisy Text ---\")\n",
    "print(noisy_text)\n",
    "\n",
    "\n",
    "######\n",
    "### Note: For loading .docx files, you would typically do:\n",
    "\n",
    "# import docx2txt\n",
    "# try:\n",
    "#     text = docx2txt.process(\"your_document.docx\")\n",
    "#     print(text)\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading docx: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a0c7ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Raw Text Length: 1254\n",
      "--- --------------------------- ---\n",
      "--- Original Raw Text ---\n",
      "\n",
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development of computer algorithms that can learn from and make predictions on data. Algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task.\n",
      "\n",
      "Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks. A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers. Mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.\n",
      "\n",
      "In its application across business problems, machine learning is also referred to as predictive analytics.\n",
      "\n",
      "\n",
      "Machine learning rocks! It's revolutionizing the world in 2023 (and beyond!).\n",
      "Visit our site: http://example.com for more info.\n",
      "This is awesome!!! We collected 1,234 data points.\n",
      "Softbank and Google are major players.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Combine texts\n",
    "raw_text = sample_document + \"\\n\" + noisy_text\n",
    "\n",
    "print(\"Original Raw Text Length:\", len(raw_text))\n",
    "print(\"--- --------------------------- ---\")\n",
    "print(\"--- Original Raw Text ---\")\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a46d2af",
   "metadata": {},
   "source": [
    "## 03) Convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d882052a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- After Lowercasing ---\n",
      "\n",
      "machine learning (ml) is a field of study in artificial intelligence concerned with the development of computer algorithms that can learn from and make predictions on data. algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task.\n",
      "\n",
      "machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks. a subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers. mathematical optimization delivers methods, theory and application domains to the field of machine learning. data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.\n",
      "\n",
      "in its application across business problems, machine learning is also referred to as predictive analytics.\n",
      "\n",
      "\n",
      "machine learning rocks! it's revolutionizing the world in 2023 (and beyond!).\n",
      "visit our site: http://example.com for more info.\n",
      "this is awesome!!! we collected 1,234 data points.\n",
      "softbank and google are major players.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = raw_text.lower()\n",
    "\n",
    "print(\"\\n--- After Lowercasing ---\")\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400a8a72",
   "metadata": {},
   "source": [
    "## 04) Remove or fetch URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba3f5f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "machine learning (ml) is a field of study in artificial intelligence concerned with the development of computer algorithms that can learn from and make predictions on data. algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task.\n",
      "\n",
      "machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks. a subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers. mathematical optimization delivers methods, theory and application domains to the field of machine learning. data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.\n",
      "\n",
      "in its application across business problems, machine learning is also referred to as predictive analytics.\n",
      "\n",
      "\n",
      "machine learning rocks! it's revolutionizing the world in 2023 (and beyond!).\n",
      "visit our site:  for more info.\n",
      "this is awesome!!! we collected 1,234 data points.\n",
      "softbank and google are major players.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    return re.sub(url_pattern, '', text)\n",
    "\n",
    "print(remove_urls(cleaned_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40adff0",
   "metadata": {},
   "source": [
    "## 05) Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1749d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2d5c1t34n\n"
     ]
    }
   ],
   "source": [
    "# string.punctuation includes -> !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# This line creates a translation table that tells Python which characters to delete from a string.\n",
    "\n",
    "# str.maketrans(from, to, delete) creates a mapping for str.translate().\n",
    "\n",
    "#In this case:\n",
    "\n",
    "# from = '' and to = '': no character substitutions.\n",
    "\n",
    "# delete = string.punctuation: remove all punctuation characters.\n",
    "\n",
    "trans = str.maketrans(\"aeiou\", \"12345\")\n",
    "\n",
    "# Explanation:\n",
    "#\"a\" → \"1\", \"e\" → \"2\", \"i\" → \"3\", \"o\" → \"4\", \"u\" → \"5\"\n",
    "\n",
    "text = \"education\"\n",
    "translated = text.translate(trans)\n",
    "print(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6f0d1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- After Removing Punctuation ---\n",
      "\n",
      "machine learning ml is a field of study in artificial intelligence concerned with the development of computer algorithms that can learn from and make predictions on data algorithms build a mathematical model based on sample data known as training data in order to make predictions or decisions without being explicitly programmed to perform the task\n",
      "\n",
      "machine learning algorithms are used in a wide variety of applications such as email filtering and computer vision where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks a subset of machine learning is closely related to computational statistics which focuses on making predictions using computers mathematical optimization delivers methods theory and application domains to the field of machine learning data mining is a related field of study focusing on exploratory data analysis through unsupervised learning\n",
      "\n",
      "in its application across business problems machine learning is also referred to as predictive analytics\n",
      "\n",
      "\n",
      "machine learning rocks its revolutionizing the world in 2023 and beyond\n",
      "visit our site httpexamplecom for more info\n",
      "this is awesome we collected 1234 data points\n",
      "softbank and google are major players\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = cleaned_text.translate(translator)\n",
    "print(\"\\n--- After Removing Punctuation ---\")\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aff055",
   "metadata": {},
   "source": [
    "## 06) Remove or fetch numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cf9bc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- After Removing Numbers ---\n",
      "\n",
      "machine learning ml is a field of study in artificial intelligence concerned with the development of computer algorithms that can learn from and make predictions on data algorithms build a mathematical model based on sample data known as training data in order to make predictions or decisions without being explicitly programmed to perform the task\n",
      "\n",
      "machine learning algorithms are used in a wide variety of applications such as email filtering and computer vision where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks a subset of machine learning is closely related to computational statistics which focuses on making predictions using computers mathematical optimization delivers methods theory and application domains to the field of machine learning data mining is a related field of study focusing on exploratory data analysis through unsupervised learning\n",
      "\n",
      "in its application across business problems machine learning is also referred to as predictive analytics\n",
      "\n",
      "\n",
      "machine learning rocks its revolutionizing the world in  and beyond\n",
      "visit our site httpexamplecom for more info\n",
      "this is awesome we collected  data points\n",
      "softbank and google are major players\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "re.sub(pattern, replacement, text)\n",
    "→ Substitutes all matches of the pattern with the replacement string ('' here).\n",
    "\"\"\"\n",
    "\n",
    "cleaned_text = re.sub(r'\\d+', '', cleaned_text)#Removes all numbers (whole or part of numbers) from the text.\n",
    "\n",
    "\n",
    "print(\"\\n--- After Removing Numbers ---\")\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a559cba1",
   "metadata": {},
   "source": [
    "## 07) Remove extra white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e853a6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- After Removing Extra Whitespace ---\n",
      "machine learning ml is a field of study in artificial intelligence concerned with the development of computer algorithms that can learn from and make predictions on data algorithms build a mathematical model based on sample data known as training data in order to make predictions or decisions without being explicitly programmed to perform the task machine learning algorithms are used in a wide variety of applications such as email filtering and computer vision where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks a subset of machine learning is closely related to computational statistics which focuses on making predictions using computers mathematical optimization delivers methods theory and application domains to the field of machine learning data mining is a related field of study focusing on exploratory data analysis through unsupervised learning in its application across business problems machine learning is also referred to as predictive analytics machine learning rocks its revolutionizing the world in and beyond visit our site httpexamplecom for more info this is awesome we collected data points softbank and google are major players\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "print(\"\\n--- After Removing Extra Whitespace ---\")\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "448df574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Text Length: 1201\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCleaned Text Length:\", len(cleaned_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a123518",
   "metadata": {},
   "source": [
    "## 08) Sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9e91f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sentence Tokenization (NLTK) ---\n",
      "Sentence 1: \n",
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development of computer algorithms that can learn from and make predictions on data.\n",
      "Sentence 2: Algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task.\n",
      "Sentence 3: Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks.\n",
      "Sentence 4: A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers.\n",
      "Sentence 5: Mathematical optimization delivers methods, theory and application domains to the field of machine learning.\n",
      "Sentence 6: Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.\n",
      "Sentence 7: In its application across business problems, machine learning is also referred to as predictive analytics.\n",
      "Sentence 8: Machine learning rocks!\n",
      "Sentence 9: It's revolutionizing the world in 2023 (and beyond!).\n",
      "Sentence 10: Visit our site: http://example.com for more info.\n",
      "Sentence 11: This is awesome!!!\n",
      "Sentence 12: We collected 1,234 data points.\n",
      "Sentence 13: Softbank and Google are major players.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk_sentences = sent_tokenize(raw_text) # Using raw_text to show sentence boundary handling\n",
    "print(\"--- Sentence Tokenization (NLTK) ---\")\n",
    "for i, sentence in enumerate(nltk_sentences):\n",
    "    print(f\"Sentence {i+1}: {sentence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2818c",
   "metadata": {},
   "source": [
    "## 09) Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eafef0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Word Tokenization (NLTK) ---\n",
      "['machine', 'learning', 'ml', 'is', 'a', 'field', 'of', 'study', 'in', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'development', 'of', 'computer', 'algorithms', 'that', 'can']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Using NLTK (on the cleaned text)\n",
    "nltk_word_tokens = word_tokenize(cleaned_text)\n",
    "print(\"--- Word Tokenization (NLTK) ---\")\n",
    "print(nltk_word_tokens[:20]) # Print first 20 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a300f2d8",
   "metadata": {},
   "source": [
    "## 10) Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "748ebaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample NLTK Stop Words ---\n",
      "['is', 'more', 'off', \"i'll\", 'both', 'i', \"aren't\", 'it', 'such', \"you'll\"]\n",
      "\n",
      "--- NLTK Tokens after Stop Word Removal ---\n",
      "['machine', 'learning', 'ml', 'field', 'study', 'artificial', 'intelligence', 'concerned', 'development', 'computer', 'algorithms', 'learn', 'make', 'predictions', 'data', 'algorithms', 'build', 'mathematical', 'model', 'based']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get English stop words from NLTK\n",
    "nltk_stop_words = set(stopwords.words('english'))\n",
    "print(\"--- Sample NLTK Stop Words ---\")\n",
    "print(list(nltk_stop_words)[:10])\n",
    "\n",
    "# Remove stop words using NLTK tokens\n",
    "nltk_tokens_no_stopwords = [word for word in nltk_word_tokens if word not in nltk_stop_words]\n",
    "print(\"\\n--- NLTK Tokens after Stop Word Removal ---\")\n",
    "print(nltk_tokens_no_stopwords[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c933bc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'learning': 7, 'machine': 6, 'data': 6, 'algorithms': 4, 'field': 3, 'predictions': 3, 'study': 2, 'computer': 2, 'make': 2, 'mathematical': 2, 'perform': 2, 'related': 2, 'application': 2, 'ml': 1, 'artificial': 1, 'intelligence': 1, 'concerned': 1, 'development': 1, 'learn': 1, 'build': 1, 'model': 1, 'based': 1, 'sample': 1, 'known': 1, 'training': 1, 'order': 1, 'decisions': 1, 'without': 1, 'explicitly': 1, 'programmed': 1, 'task': 1, 'used': 1, 'wide': 1, 'variety': 1, 'applications': 1, 'email': 1, 'filtering': 1, 'vision': 1, 'difficult': 1, 'infeasible': 1, 'develop': 1, 'conventional': 1, 'needed': 1, 'tasks': 1, 'subset': 1, 'closely': 1, 'computational': 1, 'statistics': 1, 'focuses': 1, 'making': 1, 'using': 1, 'computers': 1, 'optimization': 1, 'delivers': 1, 'methods': 1, 'theory': 1, 'domains': 1, 'mining': 1, 'focusing': 1, 'exploratory': 1, 'analysis': 1, 'unsupervised': 1, 'across': 1, 'business': 1, 'problems': 1, 'also': 1, 'referred': 1, 'predictive': 1, 'analytics': 1, 'rocks': 1, 'revolutionizing': 1, 'world': 1, 'beyond': 1, 'visit': 1, 'site': 1, 'httpexamplecom': 1, 'info': 1, 'awesome': 1, 'collected': 1, 'points': 1, 'softbank': 1, 'google': 1, 'major': 1, 'players': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Step 5: Count word frequencies\n",
    "word_counts = Counter(nltk_tokens_no_stopwords)\n",
    "\n",
    "# Display word counts\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f81132a",
   "metadata": {},
   "source": [
    "## 11) Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6407880f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stemmed Tokens (NLTK PorterStemmer) ---\n",
      "['machin', 'learn', 'ml', 'field', 'studi', 'artifici', 'intellig', 'concern', 'develop', 'comput', 'algorithm', 'learn', 'make', 'predict', 'data', 'algorithm', 'build', 'mathemat', 'model', 'base']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "# --- Stemming with NLTK (PorterStemmer) ---\n",
    "porter_stemmer = PorterStemmer()\n",
    "stemmed_tokens = [porter_stemmer.stem(word) for word in nltk_tokens_no_stopwords] # Apply to NLTK tokens after stop word removal\n",
    "\n",
    "print(\"--- Stemmed Tokens (NLTK PorterStemmer) ---\")\n",
    "print(stemmed_tokens[:20])\n",
    "\n",
    "#Example\n",
    "#    learning -> learn\n",
    "#    machine -> machin (it can sometimes cut of last few letters of the words and thus it can also produce incorrect words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1845e9",
   "metadata": {},
   "source": [
    "## 12) Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81260a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\n",
      "--- Sentence Tokenization (spaCy) ---\n",
      "Sentence 1: \n",
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development of computer algorithms that can learn from and make predictions on data.\n",
      "Sentence 2: Algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task.\n",
      "\n",
      "\n",
      "Sentence 3: Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks.\n",
      "Sentence 4: A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers.\n",
      "Sentence 5: Mathematical optimization delivers methods, theory and application domains to the field of machine learning.\n",
      "Sentence 6: Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.\n",
      "\n",
      "\n",
      "Sentence 7: In its application across business problems, machine learning is also referred to as predictive analytics.\n",
      "\n",
      "\n",
      "\n",
      "Sentence 8: Machine learning rocks!\n",
      "Sentence 9: It's revolutionizing the world in 2023 (and beyond!).\n",
      "\n",
      "Sentence 10: Visit our site: http://example.com for more info.\n",
      "\n",
      "Sentence 11: This is awesome!!!\n",
      "Sentence 12: We collected 1,234 data points.\n",
      "\n",
      "Sentence 13: Softbank and Google are major players.\n",
      "\n",
      "\n",
      "--- Word Tokenization (spaCy) ---\n",
      "['machine', 'learning', 'ml', 'is', 'a', 'field', 'of', 'study', 'in', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'development', 'of', 'computer', 'algorithms', 'that', 'can']\n",
      "\n",
      "--- Lemmatized Tokens (spaCy) ---\n",
      "['machine', 'learn', 'ml', 'field', 'study', 'artificial', 'intelligence', 'concern', 'development', 'computer', 'algorithm', 'learn', 'prediction', 'datum', 'algorithm', 'build', 'mathematical', 'model', 'base', 'sample']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization: A more sophisticated process that uses a vocabulary and morphological analysis to return the\n",
    "# base or dictionary form of a word (known as the lemma).\n",
    "# It's slower but produces actual words (e.g., \"better\" -> \"good\").\n",
    "# Lemmatization often requires the Part-of-Speech (POS) tag of the word to be accurate.\n",
    "\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.corpus import stopwords, wordnet\n",
    "# from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def get_wordnet_pos(treebank_tag):\n",
    "#     \"\"\"Map POS tag to WordNet format.\"\"\"\n",
    "#     if treebank_tag.startswith('J'):\n",
    "#         return wordnet.ADJ\n",
    "#     elif treebank_tag.startswith('V'):\n",
    "#         return wordnet.VERB\n",
    "#     elif treebank_tag.startswith('N'):\n",
    "#         return wordnet.NOUN\n",
    "#     elif treebank_tag.startswith('R'):\n",
    "#         return wordnet.ADV\n",
    "#     else:\n",
    "#         return wordnet.NOUN  # Default POS\n",
    "\n",
    "# # POS tagging\n",
    "# pos_tagged = pos_tag(nltk_tokens_no_stopwords)\n",
    "# print(\"--- pos tag ---\")\n",
    "# print(pos_tagged[:20])\n",
    "\n",
    "# # Lemmatize using POS\n",
    "# nltk_lemmatized_tokens = [\n",
    "#     lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "#     for word, pos in pos_tagged\n",
    "# ]\n",
    "\n",
    "# print(\"\\n--- Lemmatized Tokens (NLTK WordNetLemmatizer) ---\")\n",
    "# print(nltk_lemmatized_tokens[:20])\n",
    "\n",
    "import spacy # altn. for nltk\n",
    "\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_sm\") #downloads the corresponding english spacy model for spacy version\n",
    "\n",
    "# Load the small English spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "spacy_doc_raw = nlp(raw_text) # Using raw_text\n",
    "spacy_sentences = [sent.text for sent in spacy_doc_raw.sents]\n",
    "print(\"\\n--- Sentence Tokenization (spaCy) ---\")\n",
    "for i, sentence in enumerate(spacy_sentences):\n",
    "    print(f\"Sentence {i+1}: {sentence}\")\n",
    "\n",
    "# Using spaCy (on the cleaned text)\n",
    "# spaCy's Doc object already contains tokens\n",
    "spacy_doc_cleaned = nlp(cleaned_text)\n",
    "spacy_word_tokens = [token.text for token in spacy_doc_cleaned]\n",
    "print(\"\\n--- Word Tokenization (spaCy) ---\")\n",
    "print(spacy_word_tokens[:20]) # Print first 20 tokens\n",
    "\n",
    "# --- Lemmatization with spaCy ---\n",
    "\n",
    "# Extract lemmas from spaCy Doc object (on cleaned text, filtering stop words)\n",
    "spacy_lemmatized_tokens = [token.lemma_ for token in spacy_doc_cleaned if not token.is_stop]\n",
    "\n",
    "print(\"\\n--- Lemmatized Tokens (spaCy) ---\")\n",
    "print(spacy_lemmatized_tokens[:20])\n",
    "\n",
    "# Note: spaCy's lemmatizer is generally more accurate as it uses context and\n",
    "# doesn't require manual POS tag conversion like NLTK's WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae37b9d",
   "metadata": {},
   "source": [
    "## 13) Bag of words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d6af87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Texts:\n",
      "1: machine learning rock revolutionize world beyond visit site info\n",
      "2: awesome collect data point\n",
      "3: softbank google major player\n",
      "4: predictive analytic use machine learn solve business problem\n",
      "\n",
      "BoW Feature Names: ['analytic' 'awesome' 'beyond' 'business' 'collect' 'data' 'google' 'info'\n",
      " 'learn' 'learning' 'machine' 'major' 'player' 'point' 'predictive'\n",
      " 'problem' 'revolutionize' 'rock' 'site' 'softbank' 'solve' 'use' 'visit'\n",
      " 'world']\n",
      "\n",
      "BoW Matrix (Count of words per document):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "        1, 1],\n",
       "       [0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0],\n",
       "       [1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Sample noisy texts\n",
    "texts = [\n",
    "    \"Machine learning rocks! It's revolutionizing the world in 2023 (and beyond!). Visit our site: http://example.com for more info.\",\n",
    "    \"This is awesome!!! We collected 1,234 data points.\",\n",
    "    \"Softbank and Google are major players.\",\n",
    "    \"Predictive analytics uses machine learning to solve business problems.\",\n",
    "]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Process text with SpaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Lemmatization + remove stopwords + tokens length > 1 (to skip leftover punct or spaces)\n",
    "    tokens = [token.lemma_ for token in doc if token.text not in stop_words and len(token.text) > 1]\n",
    "\n",
    "    # Join back to string\n",
    "    cleaned_text = \" \".join(tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "cleaned_texts = [clean_text(text) for text in texts]\n",
    "\n",
    "print(\"Cleaned Texts:\")\n",
    "for i, txt in enumerate(cleaned_texts, 1):\n",
    "    print(f\"{i}: {txt}\")\n",
    "\n",
    "\n",
    "# Using sklearn CountVectorizer for BoW\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(cleaned_texts)\n",
    "\n",
    "print(\"\\nBoW Feature Names:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\nBoW Matrix (Count of words per document):\")\n",
    "bow_matrix.toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7dc3eeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analytic</th>\n",
       "      <th>awesome</th>\n",
       "      <th>beyond</th>\n",
       "      <th>business</th>\n",
       "      <th>collect</th>\n",
       "      <th>data</th>\n",
       "      <th>google</th>\n",
       "      <th>info</th>\n",
       "      <th>learn</th>\n",
       "      <th>learning</th>\n",
       "      <th>...</th>\n",
       "      <th>predictive</th>\n",
       "      <th>problem</th>\n",
       "      <th>revolutionize</th>\n",
       "      <th>rock</th>\n",
       "      <th>site</th>\n",
       "      <th>softbank</th>\n",
       "      <th>solve</th>\n",
       "      <th>use</th>\n",
       "      <th>visit</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       analytic  awesome  beyond  business  collect  data  google  info  \\\n",
       "Doc_1         0        0       1         0        0     0       0     1   \n",
       "Doc_2         0        1       0         0        1     1       0     0   \n",
       "Doc_3         0        0       0         0        0     0       1     0   \n",
       "Doc_4         1        0       0         1        0     0       0     0   \n",
       "\n",
       "       learn  learning  ...  predictive  problem  revolutionize  rock  site  \\\n",
       "Doc_1      0         1  ...           0        0              1     1     1   \n",
       "Doc_2      0         0  ...           0        0              0     0     0   \n",
       "Doc_3      0         0  ...           0        0              0     0     0   \n",
       "Doc_4      1         0  ...           1        1              0     0     0   \n",
       "\n",
       "       softbank  solve  use  visit  world  \n",
       "Doc_1         0      0    0      1      1  \n",
       "Doc_2         0      0    0      0      0  \n",
       "Doc_3         1      0    0      0      0  \n",
       "Doc_4         0      1    1      0      0  \n",
       "\n",
       "[4 rows x 24 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names =vectorizer.get_feature_names_out()\n",
    "bow_values = bow_matrix.toarray()\n",
    "\n",
    "# Create a DataFrame\n",
    "bow_df = pd.DataFrame(bow_values, columns=feature_names)\n",
    "\n",
    "# Add row indices as Document IDs\n",
    "bow_df.index = [f\"Doc_{i+1}\" for i in range(len(bow_df))]\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\nTF-IDF DataFrame:\")\n",
    "bow_df.round(3)  # Optional: round for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a1cc6a",
   "metadata": {},
   "source": [
    "## 14) TF-IDF (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1bca4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Feature Names: ['analytic' 'awesome' 'beyond' 'business' 'collect' 'data' 'google' 'info'\n",
      " 'learn' 'learning' 'machine' 'major' 'player' 'point' 'predictive'\n",
      " 'problem' 'revolutionize' 'rock' 'site' 'softbank' 'solve' 'use' 'visit'\n",
      " 'world']\n",
      "\n",
      "TF-IDF Matrix (weights per word per document):\n",
      "[[0.         0.         0.34056989 0.         0.         0.\n",
      "  0.         0.34056989 0.         0.34056989 0.26850921 0.\n",
      "  0.         0.         0.         0.         0.34056989 0.34056989\n",
      "  0.34056989 0.         0.         0.         0.34056989 0.34056989]\n",
      " [0.         0.5        0.         0.         0.5        0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.5        0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.5        0.         0.         0.         0.         0.5\n",
      "  0.5        0.         0.         0.         0.         0.\n",
      "  0.         0.5        0.         0.         0.         0.        ]\n",
      " [0.36222393 0.         0.         0.36222393 0.         0.\n",
      "  0.         0.         0.36222393 0.         0.2855815  0.\n",
      "  0.         0.         0.36222393 0.36222393 0.         0.\n",
      "  0.         0.         0.36222393 0.36222393 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(cleaned_texts)\n",
    "\n",
    "print(\"TF-IDF Feature Names:\", tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\nTF-IDF Matrix (weights per word per document):\")\n",
    "print(tfidf_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "389473ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analytic</th>\n",
       "      <th>awesome</th>\n",
       "      <th>beyond</th>\n",
       "      <th>business</th>\n",
       "      <th>collect</th>\n",
       "      <th>data</th>\n",
       "      <th>google</th>\n",
       "      <th>info</th>\n",
       "      <th>learn</th>\n",
       "      <th>learning</th>\n",
       "      <th>...</th>\n",
       "      <th>predictive</th>\n",
       "      <th>problem</th>\n",
       "      <th>revolutionize</th>\n",
       "      <th>rock</th>\n",
       "      <th>site</th>\n",
       "      <th>softbank</th>\n",
       "      <th>solve</th>\n",
       "      <th>use</th>\n",
       "      <th>visit</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc_1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_4</th>\n",
       "      <td>0.362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       analytic  awesome  beyond  business  collect  data  google   info  \\\n",
       "Doc_1     0.000      0.0   0.341     0.000      0.0   0.0     0.0  0.341   \n",
       "Doc_2     0.000      0.5   0.000     0.000      0.5   0.5     0.0  0.000   \n",
       "Doc_3     0.000      0.0   0.000     0.000      0.0   0.0     0.5  0.000   \n",
       "Doc_4     0.362      0.0   0.000     0.362      0.0   0.0     0.0  0.000   \n",
       "\n",
       "       learn  learning  ...  predictive  problem  revolutionize   rock   site  \\\n",
       "Doc_1  0.000     0.341  ...       0.000    0.000          0.341  0.341  0.341   \n",
       "Doc_2  0.000     0.000  ...       0.000    0.000          0.000  0.000  0.000   \n",
       "Doc_3  0.000     0.000  ...       0.000    0.000          0.000  0.000  0.000   \n",
       "Doc_4  0.362     0.000  ...       0.362    0.362          0.000  0.000  0.000   \n",
       "\n",
       "       softbank  solve    use  visit  world  \n",
       "Doc_1       0.0  0.000  0.000  0.341  0.341  \n",
       "Doc_2       0.0  0.000  0.000  0.000  0.000  \n",
       "Doc_3       0.5  0.000  0.000  0.000  0.000  \n",
       "Doc_4       0.0  0.362  0.362  0.000  0.000  \n",
       "\n",
       "[4 rows x 24 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get feature names and TF-IDF matrix values\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_values = tfidf_matrix.toarray()\n",
    "# Create a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_values, columns=feature_names)\n",
    "\n",
    "# Add row indices as Document IDs\n",
    "tfidf_df.index = [f\"Doc_{i+1}\" for i in range(len(tfidf_df))]\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\nTF-IDF DataFrame:\")\n",
    "tfidf_df.round(3)  # Optional: roun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ce02e5",
   "metadata": {},
   "source": [
    "## 15) Part-of-Speech (POS) Tagging using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ec2313a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- POS Tagging with spaCy ---\n",
      "machine: NOUN (NN)\n",
      "learning: VERB (VBG)\n",
      "ml: PROPN (NNP)\n",
      "is: AUX (VBZ)\n",
      "a: DET (DT)\n",
      "field: NOUN (NN)\n",
      "of: ADP (IN)\n",
      "study: NOUN (NN)\n",
      "in: ADP (IN)\n",
      "artificial: ADJ (JJ)\n",
      "intelligence: NOUN (NN)\n",
      "concerned: VERB (VBN)\n",
      "with: ADP (IN)\n",
      "the: DET (DT)\n",
      "development: NOUN (NN)\n",
      "of: ADP (IN)\n",
      "computer: NOUN (NN)\n",
      "algorithms: NOUN (NNS)\n",
      "that: PRON (WDT)\n",
      "can: AUX (MD)\n",
      "learn: VERB (VB)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- POS Tagging with spaCy ---\")\n",
    "# spaCy tokens have .pos_ (coarse-grained) and .tag_ (fine-grained) attributes\n",
    "for i, token in enumerate(spacy_doc_cleaned):\n",
    "    print(f\"{token.text}: {token.pos_} ({token.tag_})\")\n",
    "    if i >= 20: break # Limit output\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "# SpaCy's POS tagging is part of its standard pipeline and is generally very accurate.\n",
    "# It provides both coarse-grained (`.pos_`) and fine-grained (`.tag_`) POS tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a28d511",
   "metadata": {},
   "source": [
    "## 16) Shoallow chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fc5bbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Noun Chunking with spaCy ---\n",
      "machine learning ml\n",
      "a field\n",
      "study\n",
      "artificial intelligence\n",
      "the development\n",
      "computer algorithms\n",
      "that\n",
      "predictions\n",
      "data algorithms\n",
      "a mathematical model\n",
      "sample data\n",
      "training data\n",
      "order\n",
      "predictions\n",
      "decisions\n",
      "the task machine learning algorithms\n",
      "a wide variety\n",
      "applications\n",
      "email filtering and computer vision\n",
      "it\n",
      "conventional algorithms\n",
      "the needed tasks\n",
      "a subset\n",
      "machine learning\n",
      "computational statistics\n",
      "which\n",
      "predictions\n",
      "computers\n",
      "mathematical optimization\n",
      "methods\n",
      "theory\n",
      "application domains\n",
      "the field\n",
      "machine learning data mining\n",
      "a related field\n",
      "study\n",
      "exploratory data analysis\n",
      "unsupervised learning\n",
      "its application\n",
      "business problems machine learning\n",
      "rocks\n",
      "its\n",
      "the world\n",
      "our site httpexamplecom\n",
      "more info\n",
      "this\n",
      "we\n",
      "data points softbank\n",
      "google\n",
      "major players\n"
     ]
    }
   ],
   "source": [
    "# Using spaCy to find noun chunks\n",
    "print(\"--- Noun Chunking with spaCy ---\")\n",
    "for chunk in spacy_doc_cleaned.noun_chunks:\n",
    "    print(chunk.text)\n",
    "\n",
    "# SpaCy's noun chunker is a simple form of shallow parsing.\n",
    "# More complex chunking (e.g., identifying verb phrases) can be done by defining patterns\n",
    "# or using libraries specifically for chunking with NLTK or spaCy extensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60ee499",
   "metadata": {},
   "source": [
    "## 17) Named Entity Recognition (NER) \n",
    "It is the process of identifying and classifying named entities in text into predefined categories such as person names, organizations, locations, dates, etc. This is crucial for extracting structured information from unstructured text.\n",
    "\n",
    "SpaCy has excellent built-in NER capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e3aea2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Named Entity Recognition (NER) with spaCy ---\n",
      "Entity: ML, Type: ORG\n",
      "Entity: 2023, Type: DATE\n",
      "Entity: 1,234, Type: CARDINAL\n",
      "Entity: Softbank, Type: ORG\n",
      "Entity: Google, Type: ORG\n"
     ]
    }
   ],
   "source": [
    "# Using spaCy for Named Entity Recognition\n",
    "print(\"--- Named Entity Recognition (NER) with spaCy ---\")\n",
    "for ent in spacy_doc_raw.ents: # Accessing .ents attribute for detected entities\n",
    "    if ent.label_ in ['ORG', 'GPE', 'PERSON', 'DATE', 'CARDINAL', 'LOC']: # Filter for common entity types\n",
    "         print(f\"Entity: {ent.text}, Type: {ent.label_}\")\n",
    "\n",
    "# SpaCy can identify various types of entities.\n",
    "# This is highly valuable for information extraction from documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26505f8",
   "metadata": {},
   "source": [
    "## 18) Vader sentiment analysis\n",
    "How VADER Works \n",
    "\n",
    "Uses a sentiment dictionary:\n",
    "\n",
    "Words like \"great\", \"amazing\" = positive\n",
    "\n",
    "Words like \"bad\", \"terrible\" = negative\n",
    "\n",
    "Applies rules:\n",
    "\n",
    "Boosts scores for ALL CAPS, exclamation marks (!), degree modifiers (\"very\", \"extremely\")\n",
    "\n",
    "Handles negations like \"not good\" → negative\n",
    "\n",
    "Calculates scores:\n",
    "\n",
    "Scores each word\n",
    "\n",
    "Applies modifiers\n",
    "\n",
    "Combines into neg, neu, pos\n",
    "\n",
    "Then computes the final compound score using a normalization formula (-1 to 1).\n",
    "\n",
    "     Compound Score Range\t        Sentiment Interpretation\n",
    "        >= 0.05\t                      Positive sentiment 👍\n",
    "        > -0.05 and < 0.05\t          Neutral sentiment 😐\n",
    "        <= -0.05\t                  Negative sentiment 👎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e34d0a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Srivathsan\n",
      "[nltk_data]     V\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62d2b874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sentiment Analysis with NLTK's VADER ---\n",
      "Sentence: Machine learning is a field of study in artificial intelligence.\n",
      "VADER Polarity Scores: {'neg': 0.0, 'neu': 0.721, 'pos': 0.279, 'compound': 0.4767}\n"
     ]
    }
   ],
   "source": [
    "# Example sentiment analysis on a sentence from our original text\n",
    "sentence_for_sentiment = \"Machine learning is a field of study in artificial intelligence.\"\n",
    "vs = analyzer.polarity_scores(sentence_for_sentiment)\n",
    "print(\"\\n--- Sentiment Analysis with NLTK's VADER ---\")\n",
    "print(f\"Sentence: {sentence_for_sentiment}\")\n",
    "print(f\"VADER Polarity Scores: {vs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4028223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Machine learning rocks! This is awesome!!!\n",
      "VADER Polarity Scores: {'neg': 0.0, 'neu': 0.487, 'pos': 0.513, 'compound': 0.7405}\n"
     ]
    }
   ],
   "source": [
    "sentence_for_sentiment_2 = \"Machine learning rocks! This is awesome!!!\"\n",
    "vs2 = analyzer.polarity_scores(sentence_for_sentiment_2)\n",
    "print(f\"Sentence: {sentence_for_sentiment_2}\")\n",
    "print(f\"VADER Polarity Scores: {vs2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d6e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7006c9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
